{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "generative_ai_disabled": true,
      "authorship_tag": "ABX9TyNOwRQmQKX6OYBZnMZTJKNG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AMX07/makemore/blob/main/makemore-1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# E01: train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?\n",
        "# E02: split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?\n",
        "# E03: use the dev set to tune the strength of smoothing (or regularization) for the trigram model - i.e. try many possibilities and see which one works best based on the dev set loss. What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve?\n",
        "# E04: we saw that our 1-hot vectors merely select a row of W, so producing these vectors explicitly feels wasteful. Can you delete our use of F.one_hot in favor of simply indexing into rows of W?\n",
        "# E05: look up and use F.cross_entropy instead. You should achieve the same result. Can you think of why we'd prefer to use F.cross_entropy instead?\n",
        "# E06: meta-exercise! Think of a fun/interesting exercise and complete it."
      ],
      "metadata": {
        "collapsed": true,
        "id": "18_UYJgaZ9El"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## E01: train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?\n",
        "\n",
        "\n",
        "#explore/read the dataset by storing the contents in variable of data structure list of strings\n",
        "\n",
        "words = open('names.txt').read().splitlines()"
      ],
      "metadata": {
        "id": "tH4p4GOUaPAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = torch.randn(3, 5, requires_grad=True)\n",
        "target = torch.randint(5, (3,), dtype=torch.int64)\n",
        "loss = F.cross_entropy(input, target)\n",
        "F.cross_entropy(input, target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t27mQkrajjiK",
        "outputId": "b366691b-e910-4e11-ed88-1633fecbab5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.2813, grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9hNW8PMjk-e",
        "outputId": "55654148-d27c-4ae1-955b-7bad9e67c140"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([4, 1, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae_ZbEesvNlr",
        "outputId": "a674f7a0-fdf2-448b-849e-e3e4eb4ceba4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['emma',\n",
              " 'olivia',\n",
              " 'ava',\n",
              " 'isabella',\n",
              " 'sophia',\n",
              " 'charlotte',\n",
              " 'mia',\n",
              " 'amelia',\n",
              " 'harper',\n",
              " 'evelyn']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NN approach\n",
        "instead of having a look-up table which stores the 'counts',\n",
        "we have weights which we get as a result of gradient descent.\n",
        "\n",
        "how to get the weights? in another words how to train the model?\n",
        "NN structure\n",
        "\n",
        "xenc @ W\n",
        "\n",
        "initiation\n",
        "start with random weights\n",
        "have a represenation of the alphabets which we can mathematically operate on.\n",
        "\n",
        "forward pass\n",
        "exnc @ W log-counts so logits\n",
        "\n",
        "counts = logits.exp easier to operate mathematically?\n",
        "prob = counts/counts.sum(1,keepdim=true)\n",
        "\n",
        "loss = -prob[torch.arange(num),ys].log().mean()\n",
        "\n",
        "backward pass\n"
      ],
      "metadata": {
        "id": "1Xfj2p-Z4dRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chs = sorted(set(\"\".join(words)))\n",
        "\n",
        "stoi = {s:i+1 for i,s in enumerate(chs)}\n",
        "stoi['.'] = 0\n",
        "itos = {i:s for s,i in stoi.items()}\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "VoxHcnU3EEMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "yrTQvi8qE_pt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create dataset\n",
        "x = []\n",
        "y = []\n",
        "for w in words:\n",
        "  # print(w)\n",
        "  chs = ['.'] + list(w) + ['.']\n",
        "\n",
        "  for ch1, ch2 in zip(chs,chs[1:]):\n",
        "    x.append(stoi[ch1])\n",
        "    y.append(stoi[ch2])\n",
        "\n",
        "x = torch.tensor(x)\n",
        "y = torch.tensor(y)\n",
        "num = x.nelement()\n",
        "print(num)\n",
        "xenc = F.one_hot(x,27).float()\n",
        "\n",
        "\n",
        "# initate random W weights and use a generator with a seed to get the same results back!\n",
        "g = torch.Generator().manual_seed(2147483647)\n",
        "W = torch.rand((27,27), generator = g, requires_grad = True,dtype=torch.float)\n",
        "# in this 27 by 27 matrix, we have\n",
        "# so what xenc @ W reall does is that it selects a row of W, how do you select a row of W using indexing? W[x]\n",
        "# so which rows should be selected? ith rows where i is in x.items()\n",
        "#\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "336-MoCDSIlR",
        "outputId": "c2f6e97b-dda2-44ed-cbff-a87a6563e490"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "228146\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for k in range(500):\n",
        "  #forward\n",
        "  logits = W[x]\n",
        "  counts = logits.exp()\n",
        "  probs = counts/counts.sum(1,keepdim=True)\n",
        "  loss = - probs[torch.arange(num),y].log().mean()\n",
        "  print(loss.item())\n",
        "  #backward\n",
        "  W.grad = None\n",
        "  loss.backward()\n",
        "  #update params\n",
        "  W.data += -50 * W.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "XF8z5TEGa4j8",
        "outputId": "a75c860e-b0d3-459b-82d0-79a63fdb2657"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.4564311504364014\n",
            "2.4564263820648193\n",
            "2.4564216136932373\n",
            "2.4564168453216553\n",
            "2.4564123153686523\n",
            "2.456407308578491\n",
            "2.456402540206909\n",
            "2.4563980102539062\n",
            "2.456393241882324\n",
            "2.456388473510742\n",
            "2.4563839435577393\n",
            "2.4563794136047363\n",
            "2.4563748836517334\n",
            "2.4563701152801514\n",
            "2.4563655853271484\n",
            "2.4563610553741455\n",
            "2.4563565254211426\n",
            "2.4563519954681396\n",
            "2.4563474655151367\n",
            "2.456342935562134\n",
            "2.4563381671905518\n",
            "2.456333875656128\n",
            "2.456329584121704\n",
            "2.456325054168701\n",
            "2.4563207626342773\n",
            "2.4563162326812744\n",
            "2.4563119411468506\n",
            "2.4563076496124268\n",
            "2.456303119659424\n",
            "2.456299066543579\n",
            "2.4562947750091553\n",
            "2.4562904834747314\n",
            "2.4562859535217285\n",
            "2.456281900405884\n",
            "2.456277370452881\n",
            "2.456273317337036\n",
            "2.4562692642211914\n",
            "2.4562652111053467\n",
            "2.4562606811523438\n",
            "2.45625638961792\n",
            "2.456252336502075\n",
            "2.4562480449676514\n",
            "2.4562439918518066\n",
            "2.456239700317383\n",
            "2.456235885620117\n",
            "2.4562315940856934\n",
            "2.4562275409698486\n",
            "2.456223487854004\n",
            "2.4562196731567383\n",
            "2.4562156200408936\n",
            "2.456211805343628\n",
            "2.456207513809204\n",
            "2.4562034606933594\n",
            "2.4561994075775146\n",
            "2.456195592880249\n",
            "2.4561915397644043\n",
            "2.4561879634857178\n",
            "2.456183671951294\n",
            "2.456179618835449\n",
            "2.4561760425567627\n",
            "2.456171989440918\n",
            "2.4561681747436523\n",
            "2.4561643600463867\n",
            "2.4561607837677\n",
            "2.4561567306518555\n",
            "2.456153154373169\n",
            "2.456148862838745\n",
            "2.4561455249786377\n",
            "2.456141233444214\n",
            "2.4561376571655273\n",
            "2.4561338424682617\n",
            "2.456130027770996\n",
            "2.4561264514923096\n",
            "2.456122636795044\n",
            "2.4561190605163574\n",
            "2.456115245819092\n",
            "2.456111431121826\n",
            "2.4561078548431396\n",
            "2.4561045169830322\n",
            "2.4561007022857666\n",
            "2.456096887588501\n",
            "2.4560933113098145\n",
            "2.456089496612549\n",
            "2.4560861587524414\n",
            "2.456082344055176\n",
            "2.4560787677764893\n",
            "2.4560751914978027\n",
            "2.456071615219116\n",
            "2.456068515777588\n",
            "2.4560649394989014\n",
            "2.456061601638794\n",
            "2.4560577869415283\n",
            "2.456054449081421\n",
            "2.4560508728027344\n",
            "2.456047296524048\n",
            "2.4560439586639404\n",
            "2.456040382385254\n",
            "2.4560368061065674\n",
            "2.456033706665039\n",
            "2.4560303688049316\n",
            "2.456026792526245\n",
            "2.4560234546661377\n",
            "2.456019878387451\n",
            "2.4560165405273438\n",
            "2.4560134410858154\n",
            "2.456009864807129\n",
            "2.4560067653656006\n",
            "2.456003427505493\n",
            "2.456000328063965\n",
            "2.4559967517852783\n",
            "2.455993175506592\n",
            "2.4559903144836426\n",
            "2.455986738204956\n",
            "2.4559834003448486\n",
            "2.4559805393218994\n",
            "2.455977201461792\n",
            "2.4559738636016846\n",
            "2.455970525741577\n",
            "2.455967426300049\n",
            "2.4559643268585205\n",
            "2.455960988998413\n",
            "2.4559576511383057\n",
            "2.4559547901153564\n",
            "2.455951452255249\n",
            "2.4559483528137207\n",
            "2.4559452533721924\n",
            "2.455942153930664\n",
            "2.455939292907715\n",
            "2.4559359550476074\n",
            "2.455932855606079\n",
            "2.455929756164551\n",
            "2.4559268951416016\n",
            "2.455923557281494\n",
            "2.455920457839966\n",
            "2.4559171199798584\n",
            "2.455914258956909\n",
            "2.45591139793396\n",
            "2.4559080600738525\n",
            "2.4559051990509033\n",
            "2.455902338027954\n",
            "2.455899477005005\n",
            "2.4558961391448975\n",
            "2.455893039703369\n",
            "2.455890417098999\n",
            "2.4558873176574707\n",
            "2.4558842182159424\n",
            "2.455881357192993\n",
            "2.455878496170044\n",
            "2.4558756351470947\n",
            "2.4558725357055664\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2539960038.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;31m#backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m   \u001b[0;31m#update params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m50\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             )\n\u001b[0;32m--> 625\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "W.size() # 27,27\n",
        "# W[torch.arange(2)]\n",
        "logits = xenc @ W\n",
        "logits.shape #228146, 27\n",
        "l = x.tolist()\n",
        "W[l]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGtmaBeedRCD",
        "outputId": "09699fda-48fa-4871-a050-6cf2988079d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.7081, 0.3542, 0.1054,  ..., 0.9928, 0.3419, 0.2070],\n",
              "        [0.2629, 0.1921, 0.0504,  ..., 0.8833, 0.8975, 0.5053],\n",
              "        [0.4098, 0.4807, 0.6182,  ..., 0.9564, 0.3926, 0.8961],\n",
              "        ...,\n",
              "        [0.7362, 0.3015, 0.2502,  ..., 0.9793, 0.3303, 0.8327],\n",
              "        [0.2973, 0.0136, 0.3904,  ..., 0.4831, 0.3783, 0.8224],\n",
              "        [0.4570, 0.9394, 0.9213,  ..., 0.0919, 0.2963, 0.9742]],\n",
              "       grad_fn=<IndexBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upHrPXu2hdFI",
        "outputId": "fae0eb3e-eb2a-4bb3-b2f3-e14b0806702d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.7081, 0.3542, 0.1054,  ..., 0.9928, 0.3419, 0.2070],\n",
              "        [0.2629, 0.1921, 0.0504,  ..., 0.8833, 0.8975, 0.5053],\n",
              "        [0.4098, 0.4807, 0.6182,  ..., 0.9564, 0.3926, 0.8961],\n",
              "        ...,\n",
              "        [0.7362, 0.3015, 0.2502,  ..., 0.9793, 0.3303, 0.8327],\n",
              "        [0.2973, 0.0136, 0.3904,  ..., 0.4831, 0.3783, 0.8224],\n",
              "        [0.4570, 0.9394, 0.9213,  ..., 0.0919, 0.2963, 0.9742]],\n",
              "       grad_fn=<MmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "  ix1 = 0\n",
        "  ix2 = 0\n",
        "  out = []\n",
        "  while True:\n",
        "    x1 = torch.tensor([ix1])\n",
        "    x2 = torch.tensor([ix2])\n",
        "    x1enc = F.one_hot(x1,num_classes=27)\n",
        "    x2enc = F.one_hot(x2,num_classes=27)\n",
        "    xenc = torch.cat((x1enc,x2enc),1).float()\n",
        "    logits = xenc @ W\n",
        "    counts = logits.exp()\n",
        "    p = counts / counts.sum(1, keepdim=True)\n",
        "    ix3 = (torch.multinomial(p, num_samples=1,replacement=True, generator=g)).item()\n",
        "    out.append(itos[ix3])\n",
        "    if ix3 == 0:\n",
        "      break\n",
        "    ix1 , ix2 = ix2, ix3\n",
        "  print(''.join(out))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZiXtL8sJh8Jo",
        "outputId": "615a9e0b-9154-4647-c58b-a58dba15cb33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yandan.\n",
            "zaylizap.\n",
            "owrnayl.\n",
            "ulee.\n",
            "dezenleret.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "HW-2, spliting datasets into dev, training and test!!"
      ],
      "metadata": {
        "id": "FL_O1XJgw_rC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = torch.randperm(len(words))\n",
        "n_train = int(0.8*len(words))\n",
        "n_dev = int(0.1*len(words))\n",
        "\n",
        "train = [words[i] for i in n[:n_train]]\n",
        "dev = [words[i] for i in n[n_train:n_train+n_dev]]\n",
        "test = [words[i] for i in n[n_train+n_dev:]]\n"
      ],
      "metadata": {
        "id": "hpCyK8qvtYs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## E03: use the dev set to tune the strength of smoothing (or regularization) for the trigram model\n",
        "#- i.e. try many possibilities and see which one works best based on the dev set loss.\n",
        "#What patterns can you see in the train and dev set loss as you tune this strength?\n",
        "#Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve?\n"
      ],
      "metadata": {
        "id": "wgthPQ7Yw7F1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#traing a trigram model with regularization\n",
        "\n",
        "\n",
        "# gotta create dataset!\n",
        "x1 = []\n",
        "x2 = []\n",
        "y  = []\n",
        "\n",
        "for w in train:\n",
        "  chs = ['.'] + list(w) + ['.']\n",
        "  for ch1, ch2, ch3 in zip(chs,chs[1:],chs[2:]):\n",
        "    x1.append(stoi[ch1])\n",
        "    x2.append(stoi[ch2])\n",
        "    y.append(stoi[ch3])\n",
        "\n",
        "x1 = torch.tensor(x1) #dtype is int\n",
        "x2 = torch.tensor(x2)\n",
        "# do we need y to be a tensor? Yes!!\n",
        "y = torch.tensor(y)\n",
        "n1 = x1.nelement()\n",
        "print(x2.nelement())\n",
        "# do we need n2? nope\n",
        "\n",
        "\n",
        "\n",
        "# one_hot_encoding for representations!\n",
        "x1enc = F.one_hot(x1,27)\n",
        "x2enc = F.one_hot(x2,27)\n",
        "\n",
        "#gotta concat them!\n",
        "xenc = torch.cat((x1enc,x2enc),dim=1).float() # shape 156807 x 54\n",
        "\n",
        "\n",
        "# initate W and g\n",
        "\n",
        "g = torch.Generator().manual_seed(214783647)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujN7Vv-G4NYN",
        "outputId": "128f2740-ba3e-4ac7-8af6-4cedb7662201"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "156912\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# evaluate the wights on dev\n",
        "\n",
        "\n",
        "# gotta create dataset!\n",
        "x1_dev = []\n",
        "x2_dev= []\n",
        "y_dev  = []\n",
        "\n",
        "for w in dev:\n",
        "  chs = ['.'] + list(w) + ['.']\n",
        "  for ch1, ch2, ch3 in zip(chs,chs[1:],chs[2:]):\n",
        "    x1_dev.append(stoi[ch1])\n",
        "    x2_dev.append(stoi[ch2])\n",
        "    y_dev.append(stoi[ch3])\n",
        "\n",
        "x1_dev = torch.tensor(x1_dev) #dtype is int\n",
        "x2_dev = torch.tensor(x2_dev)\n",
        "y_dev = torch.tensor(y_dev)\n",
        "n_dev = x1_dev.nelement()\n",
        "print(n_dev)\n",
        "\n",
        "\n",
        "\n",
        "# one_hot_encoding for representations!\n",
        "x1enc_dev = F.one_hot(x1_dev,27)\n",
        "x2enc_dev = F.one_hot(x2_dev,27)\n",
        "\n",
        "#gotta concat them!\n",
        "xenc_dev = torch.cat((x1enc_dev,x2enc_dev),dim=1).float() # shape 156807 x 54\n",
        "\n",
        "# logits_dev = xenc @ W\n",
        "# counts_dev = logits_dev .exp()\n",
        "# probs_dev = counts_dev/counts_dev.sum(dim=1,keepdim=True)\n",
        "# loss_dev = - probs_dev[torch.arange(n1),y].log().mean() + 0.000001 * (W**2).mean()\n",
        "\n",
        "#if I increase the regularization strenght the loss increase, if I decrease the loss decrease\n",
        "# W = torch.rand((54,27),generator=g,dtype=torch.float,requires_grad=True)\n",
        "# logits_dev = xenc_dev @ W\n",
        "# counts_dev = logits_dev.exp()\n",
        "# probs_dev = counts_dev/counts_dev.sum(dim=1,keepdim=True)\n",
        "# loss_dev = - probs_dev[torch.arange(n_dev),y_dev].log().mean()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "CnSbtKnCVGcA",
        "outputId": "51809d03-56d0-457f-c04f-5df6ec20d112"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19645\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model\n",
        "for reg_st in [1,0.1,0.001,0.0001,0.000001,0.0000001,0.00000001]:\n",
        "  W = torch.rand((54,27),generator=g,dtype=torch.float,requires_grad=True)\n",
        "  print(\"reg_st loss_train loss_dev\")\n",
        "  for k in range(500):\n",
        "    # forward pass\n",
        "    logits = xenc @ W\n",
        "    counts = logits.exp()\n",
        "    probs = counts/counts.sum(dim=1,keepdim=True)\n",
        "    loss = - probs[torch.arange(n1),y].log().mean() + reg_st * (W**2).mean()\n",
        "    # print(loss.data)\n",
        "\n",
        "    #backward pass\n",
        "    W.grad = None\n",
        "    loss.backward()\n",
        "\n",
        "    #update the parameters\n",
        "    W.data += - 50 * W.grad\n",
        "\n",
        "    #final loss approx 2.2529\n",
        "\n",
        "\n",
        "\n",
        "  logits_dev = xenc_dev @ W\n",
        "  counts_dev = logits_dev.exp()\n",
        "  probs_dev = counts_dev/counts_dev.sum(dim=1,keepdim=True)\n",
        "  loss_dev = - probs_dev[torch.arange(n_dev),y_dev].log().mean()\n",
        "  print(reg_st, loss,loss_dev)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "jeVXoddwQ7Ym",
        "outputId": "54688a55-72ef-4f33-846f-c6ea3427d92d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reg_st loss_train loss_dev\n",
            "1 tensor(2.5453, grad_fn=<AddBackward0>) tensor(2.3901, grad_fn=<NegBackward0>)\n",
            "reg_st loss_train loss_dev\n",
            "0.1 tensor(2.3189, grad_fn=<AddBackward0>) tensor(2.2616, grad_fn=<NegBackward0>)\n",
            "reg_st loss_train loss_dev\n",
            "0.001 tensor(2.2419, grad_fn=<AddBackward0>) tensor(2.2364, grad_fn=<NegBackward0>)\n",
            "reg_st loss_train loss_dev\n",
            "0.0001 tensor(2.2404, grad_fn=<AddBackward0>) tensor(2.2364, grad_fn=<NegBackward0>)\n",
            "reg_st loss_train loss_dev\n",
            "1e-06 tensor(2.2403, grad_fn=<AddBackward0>) tensor(2.2363, grad_fn=<NegBackward0>)\n",
            "reg_st loss_train loss_dev\n",
            "1e-07 tensor(2.2402, grad_fn=<AddBackward0>) tensor(2.2363, grad_fn=<NegBackward0>)\n",
            "reg_st loss_train loss_dev\n",
            "1e-08 tensor(2.2402, grad_fn=<AddBackward0>) tensor(2.2363, grad_fn=<NegBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#0.0000001"
      ],
      "metadata": {
        "id": "nkXXzyIhd8_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "  out = []\n",
        "  x1 = 0\n",
        "  x2 = 0\n",
        "  while True:\n",
        "    # print(out)\n",
        "    x1enc = F.one_hot(torch.tensor([x1]),num_classes=27)\n",
        "    x2enc = F.one_hot(torch.tensor([x2]),num_classes=27)\n",
        "    xenc = torch.cat((x1enc,x2enc),dim=1).float()\n",
        "    logits = xenc @ W\n",
        "    counts = logits.exp()\n",
        "    probs = counts/counts.sum(dim=1,keepdim=True)\n",
        "    x3 = torch.multinomial(probs,num_samples=1, replacement=True,generator=g).item()\n",
        "    if x3 == 0:\n",
        "      break\n",
        "    out.append(itos[x3])\n",
        "    x1,x2 = x2,x3\n",
        "  print(\"\".join(out))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KllPNaDNRIJl",
        "outputId": "b760e5b3-36b1-402d-c4e6-226106f54cd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "halanleo\n",
            "novin\n",
            "evamikarien\n",
            "yanjezien\n",
            "ialifanna\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-CGEhVj6FJ-",
        "outputId": "443deb43-0711-4501-c913-55c08e12ad8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19592\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1860358920.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x1_dev = torch.tensor(x1) #dtype is int\n",
            "/tmp/ipython-input-1860358920.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x2_dev = torch.tensor(x2)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.6682, grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the wights on test\n",
        "\n",
        "\n",
        "\n",
        "# gotta create dataset!\n",
        "x1 = []\n",
        "x2 = []\n",
        "y  = []\n",
        "\n",
        "for w in test:\n",
        "  chs = ['.'] + list(w) + ['.']\n",
        "  for ch1, ch2, ch3 in zip(chs,chs[1:],chs[2:]):\n",
        "    x1.append(stoi[ch1])\n",
        "    x2.append(stoi[ch2])\n",
        "    y.append(stoi[ch3])\n",
        "\n",
        "x1 = torch.tensor(x1) #dtype is int\n",
        "x2 = torch.tensor(x2)\n",
        "# do we need y to be a tensor? nope\n",
        "n1 = x1.nelement()\n",
        "print(x2.nelement())\n",
        "# do we need n2? nope\n",
        "\n",
        "\n",
        "\n",
        "# one_hot_encoding for representations!\n",
        "x1enc = F.one_hot(x1,27)\n",
        "x2enc = F.one_hot(x2,27)\n",
        "\n",
        "#gotta concat them!\n",
        "xenc = torch.cat((x1enc,x2enc),dim=1).float() # shape 156807 x 54\n",
        "\n",
        "logits = xenc @ W\n",
        "counts = logits.exp()\n",
        "probs = counts/counts.sum(dim=1,keepdim=True)\n",
        "#\n",
        "loss = - probs[torch.arange(n1),y].log().mean() + 0.0000001* (W**2).mean()\n",
        "\n",
        "#if I increase the regularization strenght the loss increase, if I decrease the loss decrease\n",
        "\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUuXTyww6q8F",
        "outputId": "8f071303-6f33-4dd4-e9d1-e7c8a2f9ae7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19556\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.2486, grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FJB2-4F79HII"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}